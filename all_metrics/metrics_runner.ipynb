{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook: PREPROCESS -> METRICS + CEFR + LEV + EX TYPES\n",
    "Local run. Outputs are saved into `all_metrics/` as three CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL-IN-ONE: textbook.txt -> PREPROCESS -> METRICS + CEFR + LEV + EX TYPES (PRINT ONLY)\n",
    "# LOCAL RUN, SAVE ONLY 3 CSV IN all_metrics/\n",
    "# =========================\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textstat\n",
    "import spacy\n",
    "import Levenshtein\n",
    "from deep_translator import GoogleTranslator\n",
    "from transliterate import translit\n",
    "from IPython.display import display\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"len_cefr.csv\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "OUTPUT_DIR = REPO_ROOT / \"all_metrics\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def resolve_textbook_path(repo_root: Path, explicit: Optional[str]) -> Path:\n",
    "    if explicit:\n",
    "        p = Path(explicit).expanduser()\n",
    "        if not p.is_absolute():\n",
    "            p = (repo_root / p).resolve()\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Textbook file not found: {p}\")\n",
    "        print(\"Textbook path (explicit):\", p)\n",
    "        return p\n",
    "\n",
    "    candidates = []\n",
    "    for search_dir in [repo_root, repo_root / \"input_textbooks\"]:\n",
    "        if search_dir.exists():\n",
    "            candidates.extend(sorted(search_dir.glob(\"*.txt\")))\n",
    "\n",
    "    if len(candidates) == 1:\n",
    "        print(\"Textbook path (auto):\", candidates[0])\n",
    "        return candidates[0].resolve()\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        raise FileNotFoundError(\n",
    "            \"No textbook .txt files found. Set TEXTBOOK_PATH env var or place a .txt in repo root or input_textbooks.\"\n",
    "        )\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Multiple .txt files found. Set TEXTBOOK_PATH env var to select one: \"\n",
    "        + \", \".join(str(p) for p in candidates)\n",
    "    )\n",
    "\n",
    "\n",
    "INPUT_TXT_PATH = os.environ.get(\"TEXTBOOK_PATH\")\n",
    "TXT_PATH = resolve_textbook_path(REPO_ROOT, INPUT_TXT_PATH)\n",
    "CEFR_CSV_PATH = (REPO_ROOT / \"len_cefr.csv\").resolve()\n",
    "MODEL_BUNDLE_EN_DIR = (REPO_ROOT / \"model_bundle\").resolve()\n",
    "MODEL_BUNDLE_RU_DIR = (REPO_ROOT / \"model_bundle_ru\").resolve()\n",
    "\n",
    "if not TXT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing {TXT_PATH}.\")\n",
    "if not CEFR_CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing {CEFR_CSV_PATH}. Place len_cefr.csv in {REPO_ROOT}\")\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"TXT_PATH:\", TXT_PATH)\n",
    "print(\"CEFR_CSV_PATH:\", CEFR_CSV_PATH)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "\n",
    "ENABLE_CEFR = True\n",
    "ENABLE_LEV = True\n",
    "\n",
    "# Levenshtein: production top-k for translation\n",
    "LEV_TRANSLATE_TOP_N = 2500\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError as e:\n",
    "    raise RuntimeError(\n",
    "        \"SpaCy model en_core_web_sm is missing. Install with: python -m spacy download en_core_web_sm\"\n",
    "    ) from e\n",
    "\n",
    "# =========================\n",
    "# PREPROCESS (MVP)\n",
    "# =========================\n",
    "PAGE_RE_DEFAULT = r\"^[=\\-]{2,}\\s*PAGE\\s*(\\d+)\\s*[=\\-]{2,}$\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PageBlock:\n",
    "    page_num: int\n",
    "    text_raw: str\n",
    "    text_clean: str\n",
    "    text_en: str\n",
    "    module_id: Optional[str] = None\n",
    "\n",
    "\n",
    "def normalize_whitespace(s: str) -> str:\n",
    "    s = (s or \"\").replace(\"\\ufeff\", \"\")\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = s.replace(\"\\t\", \" \")\n",
    "    s = re.sub(r\"[ \\u00A0]+\", \" \", s)\n",
    "    s = re.sub(r\"[ \\u00A0]+\\n\", \"\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def dehyphenate_linebreaks(s: str) -> str:\n",
    "    return re.sub(r\"(?<=\\w)-\\n(?=\\w)\", \"\", s)\n",
    "\n",
    "\n",
    "def looks_like_new_block(line: str) -> bool:\n",
    "    t = (line or \"\").strip()\n",
    "    if not t:\n",
    "        return True\n",
    "    if re.match(r\"^\\d+[\\.\\)]\\s+\", t):\n",
    "        return True\n",
    "    if re.match(r\"^\\d+\\s*[a-zA-Z]\\)\\s+\", t):\n",
    "        return True\n",
    "    if re.match(r\"^\\d+[a-zA-Z]\\b\", t):\n",
    "        return True\n",
    "    if re.match(r\"^[\\-\\*]\\s+\", t):\n",
    "        return True\n",
    "    if re.match(r\"^(Module|Unit|Lesson|Starter module|Модуль|Юнит|Урок)\\b\", t, re.IGNORECASE):\n",
    "        return True\n",
    "    letters = re.sub(r\"[^A-Za-zА-Яа-яЁё]\", \"\", t)\n",
    "    if letters and sum(ch.isupper() for ch in letters) / max(len(letters), 1) > 0.8 and len(letters) >= 6:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def join_wrapped_lines_preserve_paragraphs(s: str) -> str:\n",
    "    lines = s.split(\"\\n\")\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        if line.strip() == \"\":\n",
    "            out.append(\"\")\n",
    "            i += 1\n",
    "            continue\n",
    "        merged = line.rstrip()\n",
    "        j = i\n",
    "        while j + 1 < len(lines):\n",
    "            nxt = lines[j + 1]\n",
    "            if nxt.strip() == \"\":\n",
    "                break\n",
    "            if looks_like_new_block(nxt):\n",
    "                break\n",
    "            if re.search(r\"[.!?:;]$|[.!?:;][\\\"')\\]]*$\", merged.strip()):\n",
    "                break\n",
    "            merged = merged + \" \" + nxt.strip()\n",
    "            j += 1\n",
    "        out.append(merged)\n",
    "        i = j + 1\n",
    "    joined = \"\\n\".join(out)\n",
    "    joined = re.sub(r\"\\n{3,}\", \"\\n\\n\", joined)\n",
    "    return joined.strip()\n",
    "\n",
    "\n",
    "def extract_english_layer(s: str) -> str:\n",
    "    tokens = re.findall(\n",
    "        r\"[A-Za-z]+(?:'[A-Za-z]+)?|[0-9]+|[.!?,;:\\-()\\[\\]\\\"']+|\\n+\", s\n",
    "    )\n",
    "    kept = []\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"\\n\"):\n",
    "            kept.append(tok)\n",
    "            continue\n",
    "        if re.search(r\"[A-Za-z]\", tok):\n",
    "            kept.append(tok)\n",
    "        elif re.match(r\"^[0-9]+$\", tok):\n",
    "            kept.append(tok)\n",
    "        elif re.match(r\"^[.!?,;:\\-()\\[\\]\\\"']+$\", tok):\n",
    "            kept.append(tok)\n",
    "    out = \" \".join(kept)\n",
    "    out = out.replace(\" \\n \", \"\\n\").replace(\" \\n\", \"\\n\").replace(\"\\n \", \"\\n\")\n",
    "    out = re.sub(r\"[ ]{2,}\", \" \", out)\n",
    "    out = re.sub(r\"\\s+([.!?,;:])\", r\"\\1\", out)\n",
    "    out = re.sub(r\"\\n{3,}\", \"\\n\\n\", out)\n",
    "    return out.strip()\n",
    "\n",
    "\n",
    "def detect_module_id(text_clean: str) -> Optional[str]:\n",
    "    for line in (text_clean or \"\").splitlines():\n",
    "        t = line.strip()\n",
    "        m = re.match(r\"^(?:Module|Модуль)\\s+([0-9IVXLC]+)$\", t, re.IGNORECASE)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def split_into_pages(text: str, page_re: str = PAGE_RE_DEFAULT) -> List[Dict]:\n",
    "    text = normalize_whitespace(text)\n",
    "    page_re_comp = re.compile(page_re, re.MULTILINE)\n",
    "    matches = list(page_re_comp.finditer(text))\n",
    "    if not matches:\n",
    "        return [{\"page_num\": 1, \"text\": text}]\n",
    "    pages = []\n",
    "    prefix = text[: matches[0].start()].strip()\n",
    "    if prefix:\n",
    "        pages.append({\"page_num\": 1, \"text\": prefix})\n",
    "    for idx, m in enumerate(matches):\n",
    "        page_num = int(m.group(1))\n",
    "        start = m.end()\n",
    "        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)\n",
    "        page_text = text[start:end].strip()\n",
    "        pages.append({\"page_num\": page_num, \"text\": page_text})\n",
    "    return pages\n",
    "\n",
    "\n",
    "def preprocess_document(text_raw: str, page_re: str = PAGE_RE_DEFAULT):\n",
    "    pages = split_into_pages(text_raw, page_re=page_re)\n",
    "    out_pages = []\n",
    "    last_module: Optional[str] = None\n",
    "    for p in pages:\n",
    "        raw = p[\"text\"]\n",
    "        s = normalize_whitespace(raw)\n",
    "        s = dehyphenate_linebreaks(s)\n",
    "        s = join_wrapped_lines_preserve_paragraphs(s)\n",
    "        module_here = detect_module_id(s)\n",
    "        if module_here:\n",
    "            last_module = module_here\n",
    "        en = extract_english_layer(s)\n",
    "        out_pages.append(\n",
    "            PageBlock(\n",
    "                page_num=p[\"page_num\"],\n",
    "                text_raw=raw,\n",
    "                text_clean=s,\n",
    "                text_en=en,\n",
    "                module_id=last_module,\n",
    "            )\n",
    "        )\n",
    "    return out_pages\n",
    "\n",
    "\n",
    "# =========================\n",
    "# METRICS\n",
    "# =========================\n",
    "def tokenize_en(text: str) -> List[str]:\n",
    "    return re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", (text or \"\").lower())\n",
    "\n",
    "\n",
    "def compute_ttr_family(tokens: List[str], segment_len: int = 100) -> Dict:\n",
    "    n = len(tokens)\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"tokens\": 0,\n",
    "            \"types\": 0,\n",
    "            \"ttr\": np.nan,\n",
    "            \"rttr\": np.nan,\n",
    "            \"cttr\": np.nan,\n",
    "            f\"msttr_{segment_len}\": np.nan,\n",
    "        }\n",
    "    types = len(set(tokens))\n",
    "    ttr = types / n\n",
    "    rttr = types / math.sqrt(n)\n",
    "    cttr = types / math.sqrt(2 * n)\n",
    "    n_full = n // segment_len\n",
    "    msttr = (\n",
    "        float(\n",
    "            np.mean(\n",
    "                [\n",
    "                    len(set(tokens[i * segment_len : (i + 1) * segment_len]))\n",
    "                    / segment_len\n",
    "                    for i in range(n_full)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        if n_full > 0\n",
    "        else np.nan\n",
    "    )\n",
    "    return {\n",
    "        \"tokens\": n,\n",
    "        \"types\": types,\n",
    "        \"ttr\": ttr,\n",
    "        \"rttr\": rttr,\n",
    "        \"cttr\": cttr,\n",
    "        f\"msttr_{segment_len}\": msttr,\n",
    "    }\n",
    "\n",
    "\n",
    "def safe_div(a, b):\n",
    "    return a / b if b else np.nan\n",
    "\n",
    "\n",
    "def compute_textstat_metrics(text: str) -> Dict:\n",
    "    text = text or \"\"\n",
    "    words = textstat.lexicon_count(text)\n",
    "    sents = textstat.sentence_count(text)\n",
    "    syll = textstat.syllable_count(text)\n",
    "    return {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text) if words else np.nan,\n",
    "        \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text) if words else np.nan,\n",
    "        \"words_total\": words,\n",
    "        \"sentences_total\": sents,\n",
    "        \"syllables_total\": syll,\n",
    "        \"avg_words_per_sentence\": safe_div(words, sents),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics_for_text(text: str, segment_len: int = 100) -> Dict:\n",
    "    m = {}\n",
    "    m.update(compute_textstat_metrics(text))\n",
    "    m.update(compute_ttr_family(tokenize_en(text), segment_len=segment_len))\n",
    "    return m\n",
    "\n",
    "\n",
    "def compute_metrics_df_by_page(pages, segment_len=100, min_tokens=50, min_words=50) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for p in pages:\n",
    "        m = compute_metrics_for_text(p.text_en, segment_len=segment_len)\n",
    "        rows.append({\"page_num\": p.page_num, \"module_id\": p.module_id, **m})\n",
    "    df = pd.DataFrame(rows).sort_values(\"page_num\")\n",
    "    df[\"is_sparse\"] = (df[\"tokens\"] < min_tokens) | (df[\"words_total\"] < min_words)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_metrics_df_by_module(by_page: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (\n",
    "        by_page[~by_page[\"is_sparse\"]]\n",
    "        .groupby(\"module_id\", dropna=False)\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CEFR\n",
    "# =========================\n",
    "CEFR_ORDER = {\"A1\": 1, \"A2\": 2, \"B1\": 3, \"B2\": 4, \"C1\": 5, \"C2\": 6}\n",
    "\n",
    "\n",
    "def load_cefr_lexicon(csv_path: str) -> Dict[str, str]:\n",
    "    content = open(csv_path, \"r\", encoding=\"utf-8\").read()\n",
    "    if \";\" in content and \",\" not in content:\n",
    "        content = content.replace(\";\", \",\")\n",
    "    df = pd.read_csv(StringIO(content))\n",
    "\n",
    "    word_col = None\n",
    "    level_col = None\n",
    "    for col in df.columns:\n",
    "        col_norm = col.strip().lower()\n",
    "        if col_norm == \"word\":\n",
    "            word_col = col\n",
    "        if col_norm in {\"cefr level\", \"cefr_level\"}:\n",
    "            level_col = col\n",
    "    if not word_col or not level_col:\n",
    "        raise ValueError(f\"CEFR CSV must have 'Word' and 'CEFR Level' columns, got: {df.columns.tolist()}\")\n",
    "\n",
    "    word_levels = {}\n",
    "    for _, row in df.iterrows():\n",
    "        w = str(row[word_col]).strip().lower()\n",
    "        lvl = str(row[level_col]).strip().upper()\n",
    "        if not w:\n",
    "            continue\n",
    "        if w in word_levels:\n",
    "            cur = word_levels[w]\n",
    "            if CEFR_ORDER.get(lvl, 99) < CEFR_ORDER.get(cur, 99):\n",
    "                word_levels[w] = lvl\n",
    "        else:\n",
    "            word_levels[w] = lvl\n",
    "    return word_levels\n",
    "\n",
    "\n",
    "def compute_cefr_word_table(pages, word_levels: Dict[str, str], nlp) -> Tuple[pd.DataFrame, Dict]:\n",
    "    level_names = [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"]\n",
    "    lemma_freq = {}\n",
    "    lemma_pos_counts = {}\n",
    "    total_by_level = dict.fromkeys(level_names, 0)\n",
    "    total_tokens = 0\n",
    "    texts = [p.text_en or \"\" for p in pages]\n",
    "    for doc in nlp.pipe(texts, batch_size=16):\n",
    "        for tok in doc:\n",
    "            if not tok.is_alpha or tok.is_stop:\n",
    "                continue\n",
    "            lemma = tok.lemma_.lower()\n",
    "            total_tokens += 1\n",
    "            lemma_freq[lemma] = lemma_freq.get(lemma, 0) + 1\n",
    "            lemma_pos_counts.setdefault(lemma, {})\n",
    "            lemma_pos_counts[lemma][tok.pos_] = lemma_pos_counts[lemma].get(tok.pos_, 0) + 1\n",
    "            lvl = word_levels.get(lemma)\n",
    "            if lvl in total_by_level:\n",
    "                total_by_level[lvl] += 1\n",
    "    rows = []\n",
    "    for lemma, freq in lemma_freq.items():\n",
    "        lvl = word_levels.get(lemma)\n",
    "        if lvl not in CEFR_ORDER:\n",
    "            continue\n",
    "        pos_counts = lemma_pos_counts.get(lemma, {})\n",
    "        pos = max(pos_counts.items(), key=lambda x: x[1])[0] if pos_counts else \"X\"\n",
    "        rows.append({\"word\": lemma, \"level\": lvl, \"frequency\": freq, \"pos\": pos})\n",
    "    cefr_word_table = pd.DataFrame(rows)\n",
    "    if not cefr_word_table.empty:\n",
    "        cefr_word_table[\"level_order\"] = cefr_word_table[\"level\"].map(CEFR_ORDER)\n",
    "        cefr_word_table = (\n",
    "            cefr_word_table.sort_values([\"level_order\", \"frequency\"], ascending=[True, False]).drop(\n",
    "                columns=[\"level_order\"]\n",
    "            )\n",
    "        )\n",
    "    cefr_summary = {\n",
    "        \"tokens_total\": int(total_tokens),\n",
    "        \"by_level_tokens\": {lvl: int(total_by_level[lvl]) for lvl in level_names},\n",
    "        \"by_level_pct\": {\n",
    "            lvl: (total_by_level[lvl] / total_tokens) if total_tokens else None for lvl in level_names\n",
    "        },\n",
    "    }\n",
    "    return cefr_word_table, cefr_summary\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LEVENSHTEIN (words all, no cache, print only)\n",
    "# - таблица полная по словам (кроме 1-буквенных, кроме i)\n",
    "# - перевод делаем только для top-N, чтобы не умереть по лимитам\n",
    "# =========================\n",
    "def transliterate_ru_to_en(word_ru: str) -> str:\n",
    "    try:\n",
    "        return translit(str(word_ru), \"ru\", reversed=True).lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def compute_lev_words_all(pages, nlp, translate_limit: Optional[int] = None) -> Tuple[pd.DataFrame, Dict]:\n",
    "    freq: Dict[str, int] = {}\n",
    "    texts = [p.text_en or \"\" for p in pages]\n",
    "    for doc in nlp.pipe(texts, batch_size=16):\n",
    "        for tok in doc:\n",
    "            if not tok.is_alpha or tok.is_stop:\n",
    "                continue\n",
    "            lemma = tok.lemma_.lower()\n",
    "            if len(lemma) == 1 and lemma != \"i\":\n",
    "                continue\n",
    "            freq[lemma] = freq.get(lemma, 0) + 1\n",
    "    all_words = sorted(freq.keys())\n",
    "\n",
    "    to_translate = []\n",
    "    if translate_limit is None:\n",
    "        to_translate = all_words\n",
    "    else:\n",
    "        limit = int(translate_limit)\n",
    "        if limit > 0:\n",
    "            top_words = [\n",
    "                w for w, _ in sorted(freq.items(), key=lambda x: x[1], reverse=True)[:limit]\n",
    "            ]\n",
    "            to_translate = top_words\n",
    "\n",
    "    translator = GoogleTranslator(source=\"en\", target=\"ru\")\n",
    "    tr_map: Dict[str, str] = {}\n",
    "    for i, w in enumerate(to_translate, start=1):\n",
    "        try:\n",
    "            tr_map[w] = translator.translate(w)\n",
    "        except Exception:\n",
    "            tr_map[w] = \"\"\n",
    "        if i % 50 == 0:\n",
    "            time.sleep(0.2)\n",
    "    rows = []\n",
    "    for w in all_words:\n",
    "        tr_ru = tr_map.get(w, \"\")\n",
    "        tr_en = transliterate_ru_to_en(tr_ru) if tr_ru else \"\"\n",
    "        sim = Levenshtein.ratio(w, tr_en) if tr_en else np.nan\n",
    "        rows.append(\n",
    "            {\n",
    "                \"word\": w,\n",
    "                \"frequency\": int(freq[w]),\n",
    "                \"translation_ru\": tr_ru,\n",
    "                \"translation_en_translit\": tr_en,\n",
    "                \"similarity\": sim,\n",
    "            }\n",
    "        )\n",
    "    lev_words = pd.DataFrame(rows).sort_values(\"frequency\", ascending=False).reset_index(drop=True)\n",
    "    lev_summary = {\n",
    "        \"words_total\": int(len(all_words)),\n",
    "        \"translated\": int(len(to_translate)),\n",
    "        \"lev_mean_over_translated\": float(np.nanmean(lev_words[\"similarity\"])) if len(to_translate) else None,\n",
    "    }\n",
    "    return lev_words, lev_summary\n",
    "\n",
    "\n",
    "# =========================\n",
    "# EX TYPES  RU+EN ensemble with gating (PRINT ONLY)\n",
    "# =========================\n",
    "def _find_dir_by_name(root: str, name: str, max_depth: int = 4) -> Optional[str]:\n",
    "    if not os.path.exists(root):\n",
    "        return None\n",
    "    for dirpath, dirnames, _ in os.walk(root):\n",
    "        depth = os.path.relpath(dirpath, root).count(os.sep)\n",
    "        if depth > max_depth:\n",
    "            dirnames[:] = []\n",
    "            continue\n",
    "        if os.path.basename(dirpath) == name:\n",
    "            return dirpath\n",
    "    return None\n",
    "\n",
    "\n",
    "def resolve_model_bundle_dirs(\n",
    "    en_dir: str, ru_dir: str, search_root: Path\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    en_path = en_dir if os.path.exists(en_dir) else None\n",
    "    ru_path = ru_dir if os.path.exists(ru_dir) else None\n",
    "\n",
    "    if en_path and ru_path:\n",
    "        print(\"Model bundles found in provided paths.\")\n",
    "        return en_path, ru_path\n",
    "\n",
    "    print(\"Searching for model bundles in:\", search_root)\n",
    "    if not en_path:\n",
    "        en_path = _find_dir_by_name(str(search_root), \"model_bundle\")\n",
    "    if not ru_path:\n",
    "        ru_path = _find_dir_by_name(str(search_root), \"model_bundle_ru\")\n",
    "    if en_path and ru_path:\n",
    "        print(\"Model bundles found via local search.\")\n",
    "    return en_path, ru_path\n",
    "\n",
    "\n",
    "def _pick_file(bundle_dir: str, candidates: List[str], patterns: List[str]) -> Optional[str]:\n",
    "    for pat in patterns:\n",
    "        for name in candidates:\n",
    "            if re.fullmatch(pat, name, flags=re.IGNORECASE):\n",
    "                return os.path.join(bundle_dir, name)\n",
    "    for pat in patterns:\n",
    "        for name in candidates:\n",
    "            if re.search(pat, name, flags=re.IGNORECASE):\n",
    "                return os.path.join(bundle_dir, name)\n",
    "    if candidates:\n",
    "        return os.path.join(bundle_dir, sorted(candidates)[0])\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_sklearn_bundle(bundle_dir: str, label: str):\n",
    "    if not os.path.isdir(bundle_dir):\n",
    "        raise FileNotFoundError(f\"{label}: bundle dir not found: {bundle_dir}\")\n",
    "\n",
    "    files = sorted(os.listdir(bundle_dir))\n",
    "    print(f\"[{label}] bundle dir: {bundle_dir}\")\n",
    "    print(f\"[{label}] files: {files}\")\n",
    "\n",
    "    pkl_files = [f for f in files if f.lower().endswith(\".pkl\")]\n",
    "    pipeline_candidates = [f for f in pkl_files if re.search(r\"pipeline\", f, re.IGNORECASE)]\n",
    "    vectorizer_candidates = [f for f in pkl_files if re.search(r\"(vectorizer|tfidf)\", f, re.IGNORECASE)]\n",
    "    model_candidates = [\n",
    "        f for f in pkl_files if re.search(r\"(model|clf|logreg)\", f, re.IGNORECASE)\n",
    "    ]\n",
    "\n",
    "    pipeline_path = None\n",
    "    if pipeline_candidates:\n",
    "        pipeline_path = _pick_file(\n",
    "            bundle_dir,\n",
    "            pipeline_candidates,\n",
    "            [r\"^pipeline\\.pkl$\", r\"pipeline\"],\n",
    "        )\n",
    "    elif not vectorizer_candidates:\n",
    "        pipeline_path = _pick_file(\n",
    "            bundle_dir,\n",
    "            [f for f in pkl_files if f.lower() in {\"pipeline.pkl\", \"model.pkl\", \"clf.pkl\"}],\n",
    "            [r\"^pipeline\\.pkl$\", r\"^model\\.pkl$\", r\"^clf\\.pkl$\"],\n",
    "        )\n",
    "\n",
    "    if pipeline_path:\n",
    "        print(f\"[{label}] loading mode: pipeline -> {os.path.basename(pipeline_path)}\")\n",
    "        try:\n",
    "            with open(pipeline_path, \"rb\") as f:\n",
    "                pipe = pickle.load(f)\n",
    "            return None, pipe, True\n",
    "        except Exception as e:\n",
    "            print(f\"[{label}] pipeline load failed: {repr(e)}\")\n",
    "            print(f\"[{label}] dir contents: {files}\")\n",
    "            raise\n",
    "\n",
    "    if not vectorizer_candidates or not model_candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{label}: missing vectorizer/model in {bundle_dir}. Files: {files}\"\n",
    "        )\n",
    "\n",
    "    vect_path = _pick_file(\n",
    "        bundle_dir,\n",
    "        vectorizer_candidates,\n",
    "        [\n",
    "            r\"^tfidf_vectorizer\\.pkl$\",\n",
    "            r\"^vectorizer\\.pkl$\",\n",
    "            r\"tfidf\",\n",
    "            r\"vectorizer\",\n",
    "        ],\n",
    "    )\n",
    "    model_path = _pick_file(\n",
    "        bundle_dir,\n",
    "        model_candidates,\n",
    "        [r\"^logreg_model\\.pkl$\", r\"^model\\.pkl$\", r\"^clf\\.pkl$\", r\"logreg\", r\"model\", r\"clf\"],\n",
    "    )\n",
    "\n",
    "    if not vect_path or not model_path:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{label}: could not resolve vectorizer/model in {bundle_dir}. Files: {files}\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"[{label}] loading mode: vectorizer+clf -> {os.path.basename(vect_path)} + {os.path.basename(model_path)}\"\n",
    "    )\n",
    "    try:\n",
    "        with open(vect_path, \"rb\") as f:\n",
    "            vect = pickle.load(f)\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            clf = pickle.load(f)\n",
    "        return vect, clf, False\n",
    "    except Exception as e:\n",
    "        print(f\"[{label}] vectorizer/model load failed: {repr(e)}\")\n",
    "        print(f\"[{label}] dir contents: {files}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def detect_lang_simple(text: str) -> str:\n",
    "    s = text or \"\"\n",
    "    cyr = len(re.findall(r\"[А-Яа-яЁё]\", s))\n",
    "    lat = len(re.findall(r\"[A-Za-z]\", s))\n",
    "    if cyr == 0 and lat == 0:\n",
    "        return \"unknown\"\n",
    "    if cyr > lat:\n",
    "        return \"ru\"\n",
    "    if lat > cyr:\n",
    "        return \"en\"\n",
    "    return \"mixed\"\n",
    "\n",
    "\n",
    "def predict_bundle(texts: List[str], vect, clf, is_pipeline: bool):\n",
    "    if is_pipeline:\n",
    "        pred = clf.predict(texts)\n",
    "        conf = (\n",
    "            clf.predict_proba(texts).max(axis=1)\n",
    "            if hasattr(clf, \"predict_proba\")\n",
    "            else np.full(len(texts), np.nan)\n",
    "        )\n",
    "        return pred, conf\n",
    "    X = vect.transform(texts)\n",
    "    pred = clf.predict(X)\n",
    "    conf = (\n",
    "        clf.predict_proba(X).max(axis=1)\n",
    "        if hasattr(clf, \"predict_proba\")\n",
    "        else np.full(len(texts), np.nan)\n",
    "    )\n",
    "    return pred, conf\n",
    "\n",
    "\n",
    "def predict_ex_types_multilang(texts: List[str], ru_bundle, en_bundle):\n",
    "    ru_vect, ru_clf, ru_is_pipe = ru_bundle\n",
    "    en_vect, en_clf, en_is_pipe = en_bundle\n",
    "    ru_pred, ru_conf = predict_bundle(texts, ru_vect, ru_clf, ru_is_pipe)\n",
    "    en_pred, en_conf = predict_bundle(texts, en_vect, en_clf, en_is_pipe)\n",
    "    final_pred, final_conf, chosen, lang_tag = [], [], [], []\n",
    "    for i, t in enumerate(texts):\n",
    "        lang = detect_lang_simple(t)\n",
    "        lang_tag.append(lang)\n",
    "        if lang == \"ru\":\n",
    "            final_pred.append(ru_pred[i])\n",
    "            final_conf.append(float(ru_conf[i]) if not np.isnan(ru_conf[i]) else np.nan)\n",
    "            chosen.append(\"ru_lang\")\n",
    "        elif lang == \"en\":\n",
    "            final_pred.append(en_pred[i])\n",
    "            final_conf.append(float(en_conf[i]) if not np.isnan(en_conf[i]) else np.nan)\n",
    "            chosen.append(\"en_lang\")\n",
    "        else:\n",
    "            rc = float(ru_conf[i]) if not np.isnan(ru_conf[i]) else -1.0\n",
    "            ec = float(en_conf[i]) if not np.isnan(en_conf[i]) else -1.0\n",
    "            if rc >= ec:\n",
    "                final_pred.append(ru_pred[i])\n",
    "                final_conf.append(rc)\n",
    "                chosen.append(\"ru_conf\")\n",
    "            else:\n",
    "                final_pred.append(en_pred[i])\n",
    "                final_conf.append(ec)\n",
    "                chosen.append(\"en_conf\")\n",
    "    return {\n",
    "        \"ru_pred\": ru_pred,\n",
    "        \"ru_conf\": ru_conf,\n",
    "        \"en_pred\": en_pred,\n",
    "        \"en_conf\": en_conf,\n",
    "        \"final_pred\": np.array(final_pred),\n",
    "        \"final_conf\": np.array(final_conf),\n",
    "        \"chosen\": np.array(chosen),\n",
    "        \"lang\": np.array(lang_tag),\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_candidate_instructions(pages) -> pd.DataFrame:\n",
    "    \"\"\"MVP: берём строки страниц, которые похожи на инструкции.\"\"\"\n",
    "    rows = []\n",
    "    for p in pages:\n",
    "        lines = [ln.strip() for ln in (p.text_clean or \"\").splitlines()]\n",
    "        for ln in lines:\n",
    "            if not ln or len(ln) < 5:\n",
    "                continue\n",
    "            if (\n",
    "                re.match(r\"^\\d+[\\)\\.]?\\s*\", ln)\n",
    "                or re.search(\n",
    "                    r\"\\b(Match|Complete|Choose|Read|Listen|Fill|Write|Discuss|Answer|Look|Put|Find|Make)\\b\",\n",
    "                    ln,\n",
    "                    re.IGNORECASE,\n",
    "                )\n",
    "                or re.search(\n",
    "                    r\"\\b(Соотнеси|Заполни|Выбери|Прочитай|Прослушай|Напиши|Ответь|Скажи|Составь|Найди|Поставь)\\b\",\n",
    "                    ln,\n",
    "                    re.IGNORECASE,\n",
    "                )\n",
    "            ):\n",
    "                rows.append({\"page_num\": p.page_num, \"module_id\": p.module_id, \"instruction\": ln})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "text_raw = open(TXT_PATH, \"r\", encoding=\"utf-8\").read()\n",
    "pages = preprocess_document(text_raw)\n",
    "\n",
    "print(\"=== Textbook quick check ===\")\n",
    "print(\"Pages:\", len(pages))\n",
    "print(\"First page:\", pages[0].page_num, \"module_id:\", pages[0].module_id)\n",
    "print(\"\\ntext_clean sample:\\n\", pages[0].text_clean[:250])\n",
    "print(\"\\ntext_en sample:\\n\", pages[0].text_en[:250])\n",
    "\n",
    "# --- METRICS ---\n",
    "by_page = compute_metrics_df_by_page(pages)\n",
    "by_module = compute_metrics_df_by_module(by_page)\n",
    "print(\"\\n=== metrics_by_module ===\")\n",
    "display(by_module)\n",
    "\n",
    "# --- CEFR ---\n",
    "cefr_word_table = pd.DataFrame(columns=[\"word\", \"level\", \"frequency\", \"pos\"])\n",
    "if ENABLE_CEFR:\n",
    "    word_levels = load_cefr_lexicon(CEFR_CSV_PATH)\n",
    "    cefr_word_table, cefr_summary = compute_cefr_word_table(pages, word_levels, nlp)\n",
    "    print(\"\\n=== CEFR distribution (by tokens) ===\")\n",
    "    display(pd.DataFrame([cefr_summary[\"by_level_pct\"]]))\n",
    "\n",
    "# --- LEV ---\n",
    "lev_words = pd.DataFrame(\n",
    "    columns=[\"word\", \"frequency\", \"translation_ru\", \"translation_en_translit\", \"similarity\"]\n",
    ")\n",
    "if ENABLE_LEV:\n",
    "    lev_words, lev_summary = compute_lev_words_all(pages, nlp, translate_limit=LEV_TRANSLATE_TOP_N)\n",
    "    print(\"\\n=== Levenshtein summary ===\")\n",
    "    display(pd.DataFrame([lev_summary]))\n",
    "\n",
    "# --- EX TYPES ---\n",
    "exercises_df = pd.DataFrame(\n",
    "    columns=[\"page_num\", \"module_id\", \"instruction\", \"lang\", \"chosen\", \"pred_label\", \"pred_conf\"]\n",
    ")\n",
    "\n",
    "bundle_en_dir, bundle_ru_dir = resolve_model_bundle_dirs(\n",
    "    str(MODEL_BUNDLE_EN_DIR), str(MODEL_BUNDLE_RU_DIR), REPO_ROOT\n",
    ")\n",
    "if bundle_en_dir and bundle_ru_dir:\n",
    "    try:\n",
    "        en_bundle = load_sklearn_bundle(bundle_en_dir, \"EN\")\n",
    "        ru_bundle = load_sklearn_bundle(bundle_ru_dir, \"RU\")\n",
    "        print(\n",
    "            \"\\nExercise-type models loaded:\",\n",
    "            (\"EN pipeline\" if en_bundle[2] else \"EN vect+clf\"),\n",
    "            \"|\",\n",
    "            (\"RU pipeline\" if ru_bundle[2] else \"RU vect+clf\"),\n",
    "        )\n",
    "        ex_df = extract_candidate_instructions(pages)\n",
    "        print(\"\\n=== EX TYPES: candidates ===\")\n",
    "        print(\"Rows:\", len(ex_df))\n",
    "        display(ex_df.head(20))\n",
    "        if len(ex_df) > 0:\n",
    "            preds = predict_ex_types_multilang(\n",
    "                ex_df[\"instruction\"].astype(str).tolist(), ru_bundle, en_bundle\n",
    "            )\n",
    "            ex_df[\"lang\"] = preds[\"lang\"]\n",
    "            ex_df[\"chosen\"] = preds[\"chosen\"]\n",
    "            ex_df[\"pred_label\"] = preds[\"final_pred\"]\n",
    "            ex_df[\"pred_conf\"] = preds[\"final_conf\"]\n",
    "        exercises_df = ex_df[\n",
    "            [\"page_num\", \"module_id\", \"instruction\", \"lang\", \"chosen\", \"pred_label\", \"pred_conf\"]\n",
    "        ].copy()\n",
    "        print(\"\\n=== EX TYPES: predictions sample ===\")\n",
    "        display(exercises_df.head(30))\n",
    "        if len(exercises_df) > 0:\n",
    "            print(\"\\n=== EX TYPES: label distribution ===\")\n",
    "            display(\n",
    "                exercises_df[\"pred_label\"]\n",
    "                .value_counts()\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"label\", \"pred_label\": \"count\"})\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(\"\\nEX TYPES failed:\", repr(e))\n",
    "else:\n",
    "    print(\"\\nEX TYPES skipped: model_bundle/model_bundle_ru not found.\")\n",
    "\n",
    "# --- OUTPUT: ONLY 3 CSV ---\n",
    "OUTPUT_CEFR = OUTPUT_DIR / \"cefr_word_table.csv\"\n",
    "OUTPUT_LEV = OUTPUT_DIR / \"lev_words.csv\"\n",
    "OUTPUT_EX = OUTPUT_DIR / \"exercises.csv\"\n",
    "\n",
    "cefr_word_table.to_csv(OUTPUT_CEFR, index=False)\n",
    "lev_words.to_csv(OUTPUT_LEV, index=False)\n",
    "exercises_df.to_csv(OUTPUT_EX, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"-\", OUTPUT_CEFR)\n",
    "print(\"-\", OUTPUT_LEV)\n",
    "print(\"-\", OUTPUT_EX)\n",
    "\n",
    "print(\"\\n=== OUTPUT PREVIEW: cefr_word_table (head 10) ===\")\n",
    "display(cefr_word_table.head(10))\n",
    "print(\"\\n=== OUTPUT PREVIEW: lev_words (head 10) ===\")\n",
    "display(lev_words.head(10))\n",
    "print(\"\\n=== OUTPUT PREVIEW: exercises (head 10) ===\")\n",
    "display(exercises_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}